{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Thống kê số lượng từ trong từng bộ Data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import json\n",
    "from modelling import NER\n",
    "from define_name import PATH_MODEL, PATH_CONFIG, FLAG_STRICT\n",
    "\n",
    "from dataloader import BertDataLoader"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def loading(config, PATH):\n",
    "    start = time.time()\n",
    "    print(\"1. Loading some package\")\n",
    "    ner = NER(config)\n",
    "    print(f\"===== Done !!! =====Time: {time.time() -start:.4} s =========\")\n",
    "    print('2.Load model')\n",
    "    start = time.time()\n",
    "    ner.load_model(PATH)\n",
    "    print(f\"===== Done !!! =====Time: {time.time() -start:.4} s =========\")\n",
    "    return ner"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "with open(PATH_CONFIG, 'r', encoding= 'utf-8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "ner = loading(config, PATH_MODEL)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1. Loading some package\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of XLMRobertaModel were not initialized from the model checkpoint at pretrain/xlmr_Model and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "===== Done !!! =====Time: 5.366 s =========\n",
      "2.Load model\n",
      "===== Done !!! =====Time: 2.125 s =========\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create data loader"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "dir_path = '/Users/phamvanmanh/Documents/GitHub/NER_VLSP2021/dataset/test_update_10t01.pkl'\n",
    "data_loaded, _ = ner.dataloader.create_dataloader(dir = dir_path, is_train =  False)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Predict real sententce "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "rs = ner.predict(\"We present the first multi-task learning model – named PhoNLP – for joint Vietnamese part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT (Nguyen and Nguyen, 2020) for each task independently. We publicly release PhoNLP as an open-source toolkit under the Apache License 2.0. Although we specify PhoNLP for Vietnamese, our PhoNLP training and evaluation command scripts in fact can directly work for other languages that have a pre-trained BERT-based language model and gold annotated corpora available for the three tasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future NLP research and applications to not only Vietnamese but also the other languages. Our PhoNLP is available at https://github.com/VinAIResearch/PhoNLP\")\n",
    "print(\"Chưa sử dụng batch size\")\n",
    "print(\"Thời lượng {} dự đoán (bao gồm pre & post) 1 câu\".format(rs[\"predict_time\"]))\n",
    "print(\"Thời lượng {} dự đoán 1 câu\".format(rs[\"model_time\"]))\n",
    "print(\"số lượng {} sentences dự đoán\".format(rs[\"subcut\"]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Chưa sử dụng batch size\n",
      "Thời lượng 2.3040971755981445 dự đoán (bao gồm pre & post) 1 câu\n",
      "Thời lượng 2.301755905151367 dự đoán 1 câu\n",
      "số lượng 12 sentences dự đoán\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "rs = ner.predict(\"We present the first multi-task learning model – named PhoNLP – for joint Vietnamese part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT (Nguyen and Nguyen, 2020) for each task independently. We publicly release PhoNLP as an open-source toolkit under the Apache License 2.0. Although we specify PhoNLP for Vietnamese, our PhoNLP training and evaluation command scripts in fact can directly work for other languages that have a pre-trained BERT-based language model and gold annotated corpora available for the three tasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future NLP research and applications to not only Vietnamese but also the other languages. Our PhoNLP is available at https://github.com/VinAIResearch/PhoNLP\")\n",
    "print(\"Chưa sử dụng batch size\")\n",
    "print(\"Thời lượng {} dự đoán (bao gồm pre & post) 1 câu\".format(rs[\"predict_time\"]))\n",
    "print(\"Thời lượng {} dự đoán 1 câu\".format(rs[\"model_time\"]))\n",
    "print(\"số lượng {} sentences dự đoán\".format(rs[\"subcut\"]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### INTEFERENT TIME (SPEED)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import torch\n",
    "\n",
    "avg_token_batchs = []\n",
    "count = 0\n",
    "epoch = 20\n",
    "\n",
    "for bacth in data_loaded:\n",
    "    b_input, b_mask, b_target = bacth\n",
    "    avg_token_batch = int(torch.sum(b_mask))/data_loaded.batch_size\n",
    "    avg_token_batchs.append(avg_token_batch)\n",
    "    \n",
    "    if count == epoch:\n",
    "        break\n",
    "    count += 1\n",
    "\n",
    "rs_evalue = ner.evaluate(data_loaded, epoch= epoch)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "print(\"lượng token trung bình:{}\".format(sum(avg_token_batchs)/len(avg_token_batchs)))\n",
    "print(\"dự đoán {} sentences trong 1s sử dụng CPU\".format(data_loaded.batch_size*epoch/rs_evalue[\"batch_time\"]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "lượng token trung bình:23.523809523809526\n",
      "dự đoán 7.042997614334391 sentences trong 1s sử dụng CPU\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### ACCURACY"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "epoch = 10\n",
    "rs_evalue = ner.evaluate(data_loaded, epoch= epoch, strict = FLAG_STRICT[\"MIN\"])\n",
    "print(\"accuracy by word f1-score: {}\".format(rs_evalue[\"f1\"]))\n",
    "rs_evalue = ner.evaluate(data_loaded, epoch= epoch, strict = FLAG_STRICT[\"MAX\"])\n",
    "print(\"accuracy by group word f1-score: {}\".format(rs_evalue[\"f1\"]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "accuracy by word f1-score: 0.9210098528470911\n",
      "accuracy by group word f1-score: 0.7359104155442828\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Nhận xét:\n",
    "- time inference phụ thuộc vào max len\n",
    "- sủ dụng bacth size  phụ thuộc vào max len"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "from dataloader_real import BertDataLoaderReal\n",
    "data = \"We present the first multi-task learning model – named PhoNLP – for joint Vietnamese part-of-speech (POS) tagging, named entity recognition (NER) and dependency parsing. Experiments on Vietnamese benchmark datasets show that PhoNLP produces state-of-the-art results, outperforming a single-task learning approach that fine-tunes the pre-trained Vietnamese language model PhoBERT (Nguyen and Nguyen, 2020) for each task independently. We publicly release PhoNLP as an open-source toolkit under the Apache License 2.0. Although we specify PhoNLP for Vietnamese, our PhoNLP training and evaluation command scripts in fact can directly work for other languages that have a pre-trained BERT-based language model and gold annotated corpora available for the three tasks of POS tagging, NER and dependency parsing. We hope that PhoNLP can serve as a strong baseline and useful toolkit for future NLP research and applications to not only Vietnamese but also the other languages. Our PhoNLP is available at https://github.com/VinAIResearch/PhoNLP\"\n",
    "data = data.split()\n",
    "\n",
    "data_loaded = ner.dataloaderreal.create_dataloader(data, is_train =  False)\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.12 64-bit ('akb_env': conda)"
  },
  "interpreter": {
   "hash": "04c424d4e447dcc63022a4e10e2736fefb2a280ac61ba013b1f52c8c20288e60"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}